{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# Spark session optimization\n",
    "\n"
   ],
   "id": "8df1267ef1376a97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T14:52:31.116880Z",
     "start_time": "2025-07-28T14:52:31.031255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf"
   ],
   "id": "72abfb962b4bbbd2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Understand spark memory management\n",
    "\n",
    "As we explained before, spark has different mode:\n",
    "- local mode\n",
    "- standalone\n",
    "- yarn\n",
    "- k8s\n",
    "\n",
    "The memory management is quite different for each mode.\n",
    "\n",
    "### 1.1 Local mode\n",
    "\n",
    "In local mode, all Spark components (driver and executors) run inside **a single JVM process** on your machine. `unlike YARN or Kubernetes where resources are containerized and limited.`\n",
    "A Spark driver/executor has two memory parts:\n",
    "- JVM heap memory:\n",
    "- OffHeap memory: stores `JVM metaspace, thread stacks, off-heap buffers (if enabled), native libs, Python/R processes in PySpark/SparkR`\n",
    "\n",
    "```text\n",
    "+------------------+        +-------------------+\n",
    "| JVM Heap Memory  |        | Off-Heap Memory   |\n",
    "|  - Objects       |        |  - Shuffle pages  |\n",
    "|  - Small buffers |        |  - Column batches |\n",
    "| GC managed       |        | Manual free()     |\n",
    "+------------------+        +-------------------+\n",
    "        ↑                            ↑\n",
    "     Garbage                       Tungsten\n",
    "     Collector                     MemoryMgr\n",
    "```\n",
    "\n",
    "#### 1.1.1 OffHeap memory size\n",
    "\n",
    "By default, Spark sets :\n",
    "- `spark.driver.memoryOverhead` to `max(384 MB, 0.10 * spark.driver.memory)`\n",
    "- `spark.executor.memoryOverhead` to `max(384 MB, 0.10 * spark.executor.memory)`\n",
    "\n",
    "For example, if I set `spark.driver.memory = 16GB`, then the default `overhead = 1.6GB`.\n",
    "\n",
    "#### 1.1.2 Use OffHeap memory explicitly\n",
    "\n",
    "By default, spark will use `Heap memory to store shuffle page and cache`. To work with large dataset, we need to allocate large JVM heaps. But Large JVM heaps (e.g., 20+ GB) cause **longer garbage collection pauses**. In this kind of situation, spark allows us to use OffHeap memory to store shuffle page and cache.\n",
    "\n",
    "\n",
    "**Off-heap memory can help in cases:**\n",
    "- Storing big data buffers (especially serialized blocks) off-heap: avoids longer garbage collection pauses\n",
    "- Faster shuffle and caching: `Spark’s Tungsten engine` uses binary memory layouts that are well-suited for off-heap.\n",
    "- Optimize size: Off-heap avoids object overhead (~16 bytes per object in heap).\n",
    "- Columnar processing: Off-heap buffers align better with SIMD operations, making columnar execution (Arrow, Parquet) faster.\n",
    "\n",
    "\n",
    "Below is an example of how to enable offHeap memory\n",
    "```shell\n",
    "# to enable spark to use offheap memory\n",
    "spark.memory.offHeap.enabled = true\n",
    "# when offheap enabled, the size must be set\n",
    "spark.memory.offHeap.size = 4g\n",
    "\n",
    "# we can also overwrite the default value of memory overhead\n",
    "spark.driver.memoryOverhead = 2g\n",
    "```\n",
    "\n",
    "> The above config has a problem. Because the offHeap = 4g, and memoryOverHead = 2g. When a big join or groupby is executed, a OOM may happen.\n",
    "> Because OffHeap is a part of the memoryOverhead. Best practice is memoryOverhead>=offHeap\n",
    "\n",
    "#### 1.1.3 Spark driver memory architecture\n",
    "\n",
    "```text\n",
    "+--------------------  JVM Heap (spark.driver.memory / spark.executor.memory)\n",
    "|  Reserved (JVM overhead: thread stacks, metaspace, GC)\n",
    "|\n",
    "|  Spark Unified Memory\n",
    "|    +----------------- Execution Memory\n",
    "|    |                 Storage Memory\n",
    "|\n",
    "+---------------------------------------------------------------------------------\n",
    "   ↑ JVM-managed only\n",
    "\n",
    "+--------------------  Memory Overhead (spark.driver/executor.memoryOverhead)\n",
    "| Used for:\n",
    "|    - JVM metaspace, thread stacks,\n",
    "|    - native libs,\n",
    "|    - Python/R processes in PySpark/SparkR\n",
    "+--------------------  Off-Heap Memory (spark.memory.offHeap.size)\n",
    "|  Managed by Spark Tungsten engine\n",
    "|  Used for:\n",
    "|    - Shuffle buffers\n",
    "|    - Serialized cached blocks\n",
    "|    - Columnar / Arrow / Parquet I/O\n",
    "+---------------------------------------------------------------------------------\n",
    "   ↑ OS-level RAM allocation (outside JVM heap)\n",
    "```\n",
    "#### 1.1.4 A memory config example\n",
    "\n",
    "Suppose, we have a server with 32GB RAM running Spark in local mode, here’s an optimal memory sizing recommendation balancing JVM heap and off-heap:\n",
    "\n",
    "\n",
    "- spark.driver.memory=20g: JVM heap for Spark driver. Big enough for dataset + tasks\n",
    "- spark.memory.offHeap.enabled=true: Enable off-heap to reduce JVM GC pressure\n",
    "- spark.memory.offHeap.size=6g: Off-heap buffers for shuffle, caching, serialization\n",
    "- spark.driver.memoryOverhead=6g: Soft overhead reserve to cover off-heap and native libs\n",
    "\n",
    "Why these values?\n",
    " - JVM heap (20 GB) + off-heap (6 GB) + OS & other processes (~6 GB) ≈ total 32 GB physical RAM.\n",
    " - Off-heap size (6 GB) lets Spark offload shuffle and caching buffers outside JVM heap, reducing GC pauses.\n",
    " - Leaves enough free RAM (~6 GB) for OS, background apps, and potential spikes.\n",
    "\n",
    "#### 1.1.5 No memory limit shut down in local mode\n",
    "\n",
    "### 1.2 Yarn and k8s mode\n",
    "\n",
    "## 2. Unified memory management in spark\n",
    "\n",
    "All memory reserved for the JVM heap is controlled by the spark unified memory management. It has three parts:\n",
    "- JVM overhead\n",
    "- memory for calculation\n",
    "- memory for storage\n",
    "\n",
    "### 2.1 Unified memory pool architecture\n",
    "\n",
    "\n",
    "For example, when we define `spark.driver.memory=16GB`, it means we reserved `16GB for the JVM heap`. But spark can't use 16GB in reality, we need to remove `10% for the JVM overhead`.\n",
    "Let's assume ~1.5GB overhead. So Spark can use `~14.5GB` in total. If we define `spark.memory.fraction = 0.7`(the default value is 0.6), it means spark will use 14.5GB*0.7=`10.15 for unified memory pool.`\n",
    "\n",
    "Below shows the memory architecture with the config\n",
    "\n",
    "```shell\n",
    "spark.driver.memory=16GB\n",
    "spark.memory.fraction=0.7\n",
    "```\n",
    "\n",
    "```text\n",
    "JVM Heap 16GB\n",
    "--- JVM Overhead (1.5 GB) – Non-heap\n",
    "        ┌───────────────────────────────┐\n",
    "        │ Threads, JIT, GC structures    │\n",
    "        │ Direct buffers, native libs    │\n",
    "        └───────────────────────────────┘\n",
    "--- Spark (14.5 GB after overhead)\n",
    "        ┌───────────────────────────────────────────┐\n",
    "        │ Unified Memory Pool (~10.15 GB)            │\n",
    "        │   ├── Execution Memory                     │\n",
    "        │   └── Storage Memory                       │\n",
    "        ├───────────────────────────────────────────┤\n",
    "        │ User & Other Memory (~4.35 GB)             │\n",
    "        │   ├── Python <-> JVM bridge objects        │\n",
    "        │   ├── Spark plan metadata                  │\n",
    "        │   ├── UDF intermediate data                │\n",
    "        │   ├── Temporary parse buffers              │\n",
    "        │   └── Non-managed Java objects             │\n",
    "        └───────────────────────────────────────────┘\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### 2.2 What lives outside the unified pool\n",
    "\n",
    "Why do we reserve 30% memory(by default %40)? The rest 4.35GB is for:\n",
    "\n",
    "- **Driver-side variables** (DataFrame/RDD metadata, Catalyst query plans, accumulators)\n",
    "- **Task results & status updates** before they’re sent back\n",
    "- **Broadcast variables on the driver** before being shipped to executors\n",
    "- **Temporary JVM objects** from user code (e.g., Python → Java object conversions in PySpark)\n",
    "- **UDF intermediate objects** (especially in Python UDFs with Arrow disabled)\n",
    "\n",
    "If these 30% memory run out of space, you get `java.lang.OutOfMemoryError: Java heap space` even if Spark still shows **free** execution/storage memory in the UI.\n",
    "\n",
    "> 0.7 is usually the safe upper limit in production. Going to 0.9 can be lethal unless the workload is extremely predictable."
   ],
   "id": "1c76ed9e18f9ada9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T14:53:29.401798Z",
     "start_time": "2025-07-28T14:53:05.182256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"LocalMode_memo_config\")\n",
    "    .master(\"local[*]\")\n",
    "    # JVM memory allocation\n",
    "    .config(\"spark.driver.memory\", \"16g\")  # Half of RAM for driver\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\")  # Avoid OOM on collect()\n",
    "    # Shuffle & partition tuning\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\")  # Lower than default 200\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128m\")  # Avoid large partitions in memory\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\")  # Limit shuffle buffer\n",
    "    # Unified memory management\n",
    "    .config(\"spark.memory.fraction\", \"0.7\")  # Reduce pressure on execution memory\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\")  # Smaller cache area\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "    .config(\"spark.memory.offHeap.size\", \"1g\")\n",
    "    # Spill to disk early instead of crashing\n",
    "    .config(\"spark.shuffle.spill\", \"true\")\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\n",
    "    .config(\"spark.shuffle.compress\", \"true\")\n",
    "    # optimize jvm GC\n",
    "    .config(\"spark.driver.extraJavaOptions\",\n",
    "            \"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35 -XX:+HeapDumpOnOutOfMemoryError\")\n",
    "    # Use Kryo serializer\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    # Optional: buffer size for serialization\n",
    "    .config(\"spark.kryoserializer.buffer\", \"64m\")\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"512m\")\n",
    "    .getOrCreate()\n",
    ")"
   ],
   "id": "fefd351808aefcdd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# create a spark conf\n",
    "conf = SparkConf()\n",
    "\n",
    "conf.set(\"spark.master\", \"local[*]\")  # Use all available cores\n",
    "conf.set(\"spark.app.name\", \"OptimizedLocalSparkApp\")\n",
    "\n",
    "# MEMORY\n",
    "conf.set(\"spark.driver.memory\", \"4g\")                   # Heap memory\n",
    "conf.set(\"spark.driver.memoryOverhead\", \"1024\")          # Off-heap for native libs\n",
    "\n",
    "# GC TUNING\n",
    "conf.set(\"spark.executor.extraJavaOptions\",\n",
    "         \"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent\")\n",
    "\n",
    "# OFF-HEAP (if using Arrow, Parquet, etc.)\n",
    "conf.set(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "conf.set(\"spark.memory.offHeap.size\", \"1g\")\n",
    "\n",
    "# SERIALIZATION\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "\n",
    "# SHUFFLE OPTIMIZATION\n",
    "conf.set(\"spark.shuffle.file.buffer\", \"1m\")\n",
    "conf.set(\"spark.reducer.maxSizeInFlight\", \"96m\")\n",
    "conf.set(\"spark.shuffle.io.preferDirectBufs\", \"true\")\n",
    "\n",
    "# PYTHON CONFIG\n",
    "conf.set(\"spark.python.worker.memory\", \"2g\")\n",
    "conf.set(\"spark.pyspark.python\", \"C:/Users/PLIU/Documents/git/SparkInternals/si_venv/Scripts/python.exe\")  # Replace with your Python path\n",
    "conf.set(\"spark.pyspark.driver.python\", \"C:/Users/PLIU/Documents/git/SparkInternals/si_venv/Scripts/python.exe\")\n",
    "\n",
    "# OPTIONAL: avoid memory leak from large broadcast variables\n",
    "conf.set(\"spark.cleaner.referenceTracking.blocking\", \"true\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ],
   "id": "539b7ccec0dc503d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
