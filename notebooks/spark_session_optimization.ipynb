{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# Spark session optimization\n",
    "\n"
   ],
   "id": "8df1267ef1376a97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T14:52:31.116880Z",
     "start_time": "2025-07-28T14:52:31.031255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf"
   ],
   "id": "72abfb962b4bbbd2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Understand spark memory management\n",
    "\n",
    "As we explained before, spark has different mode:\n",
    "- local mode\n",
    "- standalone\n",
    "- yarn\n",
    "- k8s\n",
    "\n",
    "The memory management is quite different for each mode.\n",
    "\n",
    "### 1.1 Local mode\n",
    "\n",
    "In local mode, all Spark components (driver and executors) run inside **a single JVM process** on your machine. `unlike YARN or Kubernetes where resources are containerized and limited.`\n",
    "A Spark driver/executor has two memory parts:\n",
    "- JVM heap memory:\n",
    "- OffHeap memory: stores `JVM metaspace, thread stacks, off-heap buffers (if enabled), native libs, Python/R processes in PySpark/SparkR`\n",
    "\n",
    "```text\n",
    "+------------------+        +-------------------+\n",
    "| JVM Heap Memory  |        | Off-Heap Memory   |\n",
    "|  - Objects       |        |  - Shuffle pages  |\n",
    "|  - Small buffers |        |  - Column batches |\n",
    "| GC managed       |        | Manual free()     |\n",
    "+------------------+        +-------------------+\n",
    "        ↑                            ↑\n",
    "     Garbage                       Tungsten\n",
    "     Collector                     MemoryMgr\n",
    "```\n",
    "\n",
    "#### 1.1.1 OffHeap memory size\n",
    "\n",
    "By default, Spark sets :\n",
    "- `spark.driver.memoryOverhead` to `max(384 MB, 0.10 * spark.driver.memory)`\n",
    "- `spark.executor.memoryOverhead` to `max(384 MB, 0.10 * spark.executor.memory)`\n",
    "\n",
    "For example, if I set `spark.driver.memory = 16GB`, then the default `overhead = 1.6GB`.\n",
    "\n",
    "#### 1.1.2 Use OffHeap memory explicitly\n",
    "\n",
    "By default, spark will use `Heap memory to store shuffle page and cache`. To work with large dataset, we need to allocate large JVM heaps. But Large JVM heaps (e.g., 20+ GB) cause **longer garbage collection pauses**. In this kind of situation, spark allows us to use OffHeap memory to store shuffle page and cache.\n",
    "\n",
    "\n",
    "**Off-heap memory can help in cases:**\n",
    "- Storing big data buffers (especially serialized blocks) off-heap: avoids longer garbage collection pauses\n",
    "- Faster shuffle and caching: `Spark’s Tungsten engine` uses binary memory layouts that are well-suited for off-heap.\n",
    "- Optimize size: Off-heap avoids object overhead (~16 bytes per object in heap).\n",
    "- Columnar processing: Off-heap buffers align better with SIMD operations, making columnar execution (Arrow, Parquet) faster.\n",
    "\n",
    "\n",
    "Below is an example of how to enable offHeap memory\n",
    "```shell\n",
    "# to enable spark to use offheap memory\n",
    "spark.memory.offHeap.enabled = true\n",
    "# when offheap enabled, the size must be set\n",
    "spark.memory.offHeap.size = 4g\n",
    "\n",
    "# we can also overwrite the default value of memory overhead\n",
    "spark.driver.memoryOverhead = 2g\n",
    "```\n",
    "\n",
    "> The above config has a problem. Because the offHeap = 4g, and memoryOverHead = 2g. When a big join or groupby is executed, a OOM may happen.\n",
    "> Because OffHeap is a part of the memoryOverhead. Best practice is memoryOverhead>=offHeap\n",
    "\n",
    "#### 1.1.3 Spark driver memory architecture\n",
    "\n",
    "```text\n",
    "+--------------------  JVM Heap (spark.driver.memory / spark.executor.memory)\n",
    "|  Reserved (JVM overhead: thread stacks, metaspace, GC)\n",
    "|\n",
    "|  Spark Unified Memory\n",
    "|    +----------------- Execution Memory\n",
    "|    |                 Storage Memory\n",
    "|\n",
    "+---------------------------------------------------------------------------------\n",
    "   ↑ JVM-managed only\n",
    "\n",
    "+--------------------  Memory Overhead (spark.driver/executor.memoryOverhead)\n",
    "| Used for:\n",
    "|    - JVM metaspace, thread stacks,\n",
    "|    - native libs,\n",
    "|    - Python/R processes in PySpark/SparkR\n",
    "+--------------------  Off-Heap Memory (spark.memory.offHeap.size)\n",
    "|  Managed by Spark Tungsten engine\n",
    "|  Used for:\n",
    "|    - Shuffle buffers\n",
    "|    - Serialized cached blocks\n",
    "|    - Columnar / Arrow / Parquet I/O\n",
    "+---------------------------------------------------------------------------------\n",
    "   ↑ OS-level RAM allocation (outside JVM heap)\n",
    "```\n",
    "### 1.1.4 A memory config example\n",
    "\n",
    "Suppose, we have a server with 32GB RAM running Spark in local mode, here’s an optimal memory sizing recommendation balancing JVM heap and off-heap:\n",
    "\n",
    "\n",
    "- spark.driver.memory=20g: JVM heap for Spark driver. Big enough for dataset + tasks\n",
    "- spark.memory.offHeap.enabled=true: Enable off-heap to reduce JVM GC pressure\n",
    "- spark.memory.offHeap.size=6g: Off-heap buffers for shuffle, caching, serialization\n",
    "- spark.driver.memoryOverhead=6g: Soft overhead reserve to cover off-heap and native libs\n",
    "\n",
    "Why these values?\n",
    " - JVM heap (20 GB) + off-heap (6 GB) + OS & other processes (~6 GB) ≈ total 32 GB physical RAM.\n",
    " - Off-heap size (6 GB) lets Spark offload shuffle and caching buffers outside JVM heap, reducing GC pauses.\n",
    " - Leaves enough free RAM (~6 GB) for OS, background apps, and potential spikes."
   ],
   "id": "1c76ed9e18f9ada9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T14:53:29.401798Z",
     "start_time": "2025-07-28T14:53:05.182256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"LocalMode_memo_config\")\n",
    "    .master(\"local[*]\")\n",
    "    # JVM memory allocation\n",
    "    .config(\"spark.driver.memory\", \"16g\")  # Half of RAM for driver\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\")  # Avoid OOM on collect()\n",
    "    # Shuffle & partition tuning\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\")  # Lower than default 200\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128m\")  # Avoid large partitions in memory\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\")  # Limit shuffle buffer\n",
    "    # Unified memory management\n",
    "    .config(\"spark.memory.fraction\", \"0.7\")  # Reduce pressure on execution memory\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\")  # Smaller cache area\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "    .config(\"spark.memory.offHeap.size\", \"1g\")\n",
    "    # Spill to disk early instead of crashing\n",
    "    .config(\"spark.shuffle.spill\", \"true\")\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\n",
    "    .config(\"spark.shuffle.compress\", \"true\")\n",
    "    # optimize jvm GC\n",
    "    .config(\"spark.driver.extraJavaOptions\",\n",
    "            \"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35 -XX:+HeapDumpOnOutOfMemoryError\")\n",
    "    # Use Kryo serializer\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    # Optional: buffer size for serialization\n",
    "    .config(\"spark.kryoserializer.buffer\", \"64m\")\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"512m\")\n",
    "    .getOrCreate()\n",
    ")"
   ],
   "id": "fefd351808aefcdd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "539b7ccec0dc503d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
